USE_HF=True srun -p llm_razor --job-name=intern7b --quotatype=auto --gres=gpu:8 --ntasks=8 --ntasks-per-node=8 --cpus-per-task=16 --kill-on-bad-exit=1 \
    python swift/cli/sft.py \
    --model_type llama3-8b-instruct \
    --model_id_or_path /mnt/petrelfs/share_data/fanqi/meta-llama/Meta-Llama-3-8B-Instruct \
    --model_revision master \
    --sft_type full \
    --template_type llama3 \
    --dtype AUTO \
    --output_dir output \
    --dataset alpaca-en \
    --train_dataset_sample -1 \
    --num_train_epochs 5 \
    --max_length 2048 \
    --check_dataset_strategy warning \
    --gradient_checkpointing true \
    --batch_size 1  \
    --weight_decay 0.1 \
    --learning_rate 2e-5 \
    --gradient_accumulation_steps 2 \
    --max_grad_norm 0.5 \
    --warmup_ratio 0.03 \
    --eval_steps 1000  \
    --save_steps 1000 \
    --save_total_limit 2 \
    --logging_steps 1 \
    --preprocess_num_proc 16  \
    --use_flash_attn True \
    --train_ds_cache /mnt/petrelfs/caoweihan/projects/swift/train_ds.pth  \
    --val_ds_cache /mnt/petrelfs/caoweihan/projects/swift/val_ds.pth  \
    --sequence_parallel_size 4  \
    --deepspeed default-zero3
